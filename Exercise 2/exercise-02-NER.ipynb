{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":841,"status":"ok","timestamp":1651851342595,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"8xp2P6Q88LNl"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1651851347781,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"EFXKWyO0q3EA"},"outputs":[],"source":["import nltk\n","from seqeval.metrics import classification_report"]},{"cell_type":"markdown","metadata":{"id":"fUqJ2UNZhRDx"},"source":["## Named Entity Recognition (NER)\n","\n","[NER](https://en.wikipedia.org/wiki/Named-entity_recognition) is a Natural Language Processing (NLP) task that consists in \n","  identifying and classifying names of entities (people, location, companies, among others) in a given sentence.\n","For instance, given the following sentence:\n","\n","```\n","Jim bought 300 shares of Acme Corp. in 2006.\n","```\n","\n","a NER system should identify and classify the three named entities as in the following:\n","\n","<font color='blue'>[Jim]<sub>Person</sub></font> bought \n","300 shares of \n","<font color='red'>[Acme Corp.]<sub>Organization</sub></font> in \n","<font color='green'>[2006]<sub>Time</sub></font>.\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"s4zAKA2wyHbX"},"source":["### IOB Tagging Format\n","In order to cast NER as a sequence tagging problem, we usually make use of the [IOB Tagging Scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)).\n","In this scheme, one *IOB tag* is attributed to each token (a token is a word or a punctuation mark) in the input sentence.\n","\n","Jim | bought | 300 | shares | of | Acme | Corp. | in | 2006 | .\n","---|---|---|---|---|---|---|---|---|---\n","`I-PER` | `O` | `O` | `O` | `O` | `I-ORG` | `I-ORG` | `O` | `I-TIME` | `O`\n","\n","As you can see, each tag identifies the type of the corresponding entity.\n","The tags `I-<type>` indicates that a token is *inside* an entity of type `<type>`.\n","Additionally, there is the *out* tag `O` indicating that the token is not part of any entity.\n","Usually, consecutive tokens classified with the same type are considered part of the same entity.\n","When there are two consecutive tokens that are part of two different entities but of the same type, we need to use a *begin* tag `B-<type>`. There is one `B-<type>` tag for each entity type.\n","Such tags indicate the first token of an entity\n","  when the previous token is part of another entity of the same type.\n","See the [Wikipedia article](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) for more details.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7DEA-8Eg3gHu"},"source":["### CoNLL-2003 Dataset\n","\n","The CoNLL-2003 dataset is one of the most used datasets for NER.\n","In the following, we show a snippet of this dataset.\n","\n","```\n","-DOCSTART- -X- -X- O\n","\n","EU NNP I-NP I-ORG\n","rejects VBZ I-VP O\n","German JJ I-NP I-MISC\n","call NN I-NP O\n","to TO I-VP O\n","boycott VB I-VP O\n","British JJ I-NP I-MISC\n","lamb NN I-NP O\n",". . O O\n","\n","Peter NNP I-NP I-PER\n","Blackburn NNP I-NP I-PER\n","\n","(...)\n","\n","-DOCSTART- -X- -X- O\n","\n","Rare NNP I-NP O\n","Hendrix NNP I-NP I-PER\n","song NN I-NP O\n","\n","(...)\n","\n","```\n","\n","Each line contains a token.\n","An empty line is used to separate sentences.\n","A line containing `-DOCSTART- -X- -X- O` is used to separate documents (for NER, usually, we disregard these document boundaries).\n","Each token contains four features separated by space:\n","```\n","word pos_tag chunk_tag ner_tag\n","```\n","The first feature `word` contains the string of the token (word or punctuation mark).\n","The second feature `pos_tag` indicates the part-of-speech of the token.\n","The third feature `chunk_tag` encodes chunking information (phrases).\n","And, finally, the last feature `ner_tag` comprises the NER annotations using IOB tagging scheme."]},{"cell_type":"markdown","metadata":{"id":"gCCyYAEZhWJH"},"source":["### Training and Evaluation Data\n","The CoNLL-2003 dataset is split into three parts: train, testa and testb.\n","The `train` split is obviously used to model training and the remaining two splits are used for model selection and testing.\n","We will use the `testa` split to evaluate our models."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":751,"status":"ok","timestamp":1651851348508,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"lE2cHwksAKro","outputId":"8ec447af-8d12-408b-e185-d9c5af60f104"},"outputs":[],"source":["# # download train and dev files\n","# !wget https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.train\n","# !wget https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.testa"]},{"cell_type":"markdown","metadata":{"id":"YZ-lOHA0hC-v"},"source":["## Encoder\n","\n","Define a simple class to encode symbols (states and observations names/strings) as integers (IDs).\n","Basically, we use two entangled data structures: a dictionary (symbol -> ID) and a list (Id -> symbol).\n","\n","Then, we can represent the HMM parameters as arrays and matrices indexed by integers (IDs) instead of strings.\n","But, at the same time, we can keep the mapping between symbol name (string) and its ID (integer)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1651851348509,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"oBpxW2TUhC-y"},"outputs":[],"source":["class Encoder:\n","    \"\"\"Encode a set of N symbols (strings) into integer IDs from {0, 1, ..., N-1}\n","\n","    The encoder can be frozen at some point and then no new symbol can be added.\n","    When a frozen encoder gets a request for an unknown symbol, it raises an\n","    exception.\n","    \"\"\"\n","\n","    def __init__(self, symbols: list = None, frozen: bool = False):\n","        \"\"\"Create a new encoder with optional list of symbols.\n","\n","        Args:\n","            symbols (list[str]): list of symbols (default is None)\n","            frozen (bool): whether the encoder includes new symbols passed to method `get_id(s)` (default is False)\n","        \"\"\"\n","        self.symbol_to_id: dict[str, int] = {}\n","        self.id_to_symbol: list[str] = []\n","        self.frozen: bool = False\n","\n","        if symbols is not None:\n","            for s in symbols:\n","                self.get_id(s)\n","\n","        if frozen:\n","            self.frozen = True\n","\n","    def get_id(self, s: str) -> int:\n","        \"\"\"return the ID (integer) corresponding to symbols `s`.\n","\n","        If symbols `s` has not been encoded already and `self.frozen == False`,\n","        then include `s` in the mapping, assign a new ID for it, and return it.\n","        If symbols `s` has not been encoded already and `self.frozen == True`,\n","        raise an exception.\n","\n","        Args:\n","            s (str): symbol to be encoded\n","\n","        Returns:\n","            int: ID of the encoded symbol\n","        \"\"\"\n","        if s not in self.symbol_to_id:\n","            if self.frozen:\n","                raise ValueError(f\"Symbol {s} not in frozen encoder {self}\")\n","\n","            new_id = len(self.id_to_symbol)\n","            self.symbol_to_id[s] = new_id\n","            self.id_to_symbol.append(s)\n","            return new_id\n","\n","        return self.symbol_to_id[s]\n","\n","    def get_symbol(self, id: int) -> str:\n","        \"\"\"return the symbols associated with `id`\n","\n","        Args:\n","            id (int): ID of the symbol\n","\n","        Returns:\n","            str: the symbol associated with the given ID\n","        \"\"\"\n","        return self.id_to_symbol[id]\n","\n","    def __repr__(self):\n","        return f\"Encoder: {self.symbol_to_id}\"\n","\n","    def __len__(self):\n","        return len(self.id_to_symbol)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1651851348510,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"sl74kPKKhkfl"},"outputs":[],"source":["# define the set of observations (x_enc) and the set of states (y_enc)\n","x_enc = Encoder()\n","y_enc = Encoder()"]},{"cell_type":"markdown","metadata":{"id":"0-_3n2_ejRW7"},"source":["### Encode a token\n","In the following function, we can apply some transformations to the word string in order to make the taggin problem easier.\n","One issue with HMM for POS tagging is that there are so many different words, \n","  and we need to convert each word into an ID (integer).\n","Specially, there are related words like `car`, `Car` and `cars`, that should almost always get the same tag, \n","  but can be encoded as different IDs."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1651851348510,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"iJ_IPk1sDP13"},"outputs":[],"source":["def encode_token(word, tag):\n","    \"\"\"Get features of a token and return a pair (x,y)\"\"\"\n","    # we can apply any transformation to the word in order to make the problem easier\n","    # word = word.lower()  # lower case\n","    # word = word[:6]  # ignore long-word ending\n","\n","    # encode word\n","    x = x_enc.get_id(word)\n","    # encode tag\n","    y = y_enc.get_id(tag)\n","\n","    return x, y"]},{"cell_type":"markdown","metadata":{"id":"UnKX1vTOj85S"},"source":["The function below iterate over a CoNLL file (train or evaluation), \n","  encode each word and its NER tag, \n","  and return two lists: `xs` and `ys`.\n","Each element in `xs` comprises of a sentence of the file (np.array of encoded words).\n","And each element in `ys` comprises the corresponding sequence of tags (np.array of encoded tags)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1651851348511,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"6LvvOrABAVEZ"},"outputs":[],"source":["def load_examples(file):\n","    xs = []\n","    ys = []\n","    with open(file) as f:\n","        x_sent = []\n","        y_sent = []\n","        for line in f:\n","            if len(line.strip()) == 0:\n","                # a blank line separates sentences\n","                if len(x_sent) > 0:\n","                    # end of a sentence\n","                    xs.append(np.array(x_sent))\n","                    ys.append(np.array(y_sent))\n","                    x_sent = []\n","                    y_sent = []\n","            elif line.startswith(\"-DOCSTART-\"):\n","                # start of a new document (ignore document limits)\n","                pass\n","            else:\n","                # a token contains four fields (separated by space):\n","                # word, POS tag, noun-phrase tag, and NER tag\n","                # the function `encode_example(...)` gets the word and NER tag\n","                # of a token and return an encoded pair (x,y)\n","                word, _, _, tag = line.split()\n","                x, y = encode_token(word, tag)\n","                x_sent.append(x)\n","                y_sent.append(y)\n","\n","    return xs, ys"]},{"cell_type":"markdown","metadata":{"id":"CG26Yy7Hkj-E"},"source":["Load train and evaluation (dev) datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1216,"status":"ok","timestamp":1651851349721,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"RgxzSm0ZFNYD"},"outputs":[],"source":["x_train, y_train = load_examples(\"eng.train\")\n","x_dev, y_dev = load_examples(\"eng.testa\")"]},{"cell_type":"markdown","metadata":{"id":"Qbs8pVXrwqkP"},"source":["Freeze the encoders to avoid bugs."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1651851349722,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"PJHRCdvcwtym"},"outputs":[],"source":["x_enc.frozen = True\n","y_enc.frozen = True"]},{"cell_type":"markdown","metadata":{"id":"9AI1Sq_dkrc-"},"source":["Number of sentences in each dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1651851349723,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"oFCX-DtTWlmx","outputId":"cc8ee2f5-91bd-4d26-d3d2-c5a2081b111f"},"outputs":[],"source":["print(f\"# train exs: {len(x_train)}\")\n","print(f\"# dev exs: {len(x_dev)}\")"]},{"cell_type":"markdown","metadata":{"id":"b1i_ggm0kxFt"},"source":["Number of unique words in the datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1651851349724,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"MQtLCzOdILy3","outputId":"10fa5b14-40e8-4a7a-c9d5-23c50678bfd1"},"outputs":[],"source":["print(f\"# unique x: {len(x_enc)}\")"]},{"cell_type":"markdown","metadata":{"id":"cntTJQuEk3NY"},"source":["Number of tags."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1651851349725,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"iyjOWeJTIRKm","outputId":"5017c849-658e-4464-a5b3-29ed531d48de"},"outputs":[],"source":["print(f\"# unique y: {len(y_enc)}\")"]},{"cell_type":"markdown","metadata":{"id":"elHzph4dlMNZ"},"source":["Visualize the tags and their corresponding IDs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1651851349726,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"W_L1XhCsIS3f","outputId":"f5a47576-0f8d-406e-ebbf-5c8632d6b539"},"outputs":[],"source":["y_enc"]},{"cell_type":"markdown","metadata":{"id":"rwI3HF7Yle1R"},"source":["Distribution of the tags.\n","We can already see an issue with NER: the output tag `O` is much more frequent than all other tags."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"elapsed":602,"status":"ok","timestamp":1651851350316,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"Hux3D_j0WAtU","outputId":"f376771e-84d0-48f7-e39d-c9f019e79401"},"outputs":[],"source":["_ = plt.hist(\n","    [y_enc.get_symbol(y) for y_seq in y_train for y in y_seq],\n","    bins=range(len(y_enc) + 1),\n","    align=\"left\",\n","    density=True,\n",")\n","_ = plt.xticks(rotation=45)"]},{"cell_type":"markdown","metadata":{"id":"SqAy_9c06eoI"},"source":["So let us have a look at the distribution of tags ignoring the `O` tag.\n","Now, we have another issue because the `B-<type>` tags are also very rare when compared to the `I-<type>` tags."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1651851350677,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"U9a-wnAO6Swc","outputId":"264f5f04-a015-4472-91ae-0c70bc6db325"},"outputs":[],"source":["_ = plt.hist(\n","    [y_enc.get_symbol(y) for y_seq in y_train for y in y_seq if y != y_enc.get_id(\"O\")],\n","    bins=range(len(y_enc) + 1),\n","    align=\"left\",\n","    density=True,\n",")\n","_ = plt.xticks(rotation=45)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1651851350906,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"28ZiwLZMxCAe","outputId":"94bc0018-a2de-4d6e-b6c4-5626d1bce2fd"},"outputs":[],"source":["from collections import Counter\n","\n","counter = Counter([y_enc.get_symbol(y) for y_seq in y_train for y in y_seq])\n","total = sum(counter.values())\n","print({tag: count / total for tag, count in counter.items()})"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qxVkudaJrSP1"},"source":["## Evaluation Script\n","NER models are usually evaluated using the CoNLL-2003 evaluation metric.\n","This metric considers [precision, recall and f1-score](https://en.wikipedia.org/wiki/Precision_and_recall).\n","In this implementation, these values are computed with respect to entities instead of individual tokens.\n","A predicted entity is considered correct only if the exact tokens are correctly identified and classified.\n","\n","We use the seqeval library to compute the CoNLL metric.\n","More specifically, we use the `classification_report` function \n","  which reports precision, recall, f1-score and support for each entity type plus three averages over the entity types: micro, macro and weighted.\n","In the following, you can see an example of this classification report.\n","The most important value in this table is the micro-averaged f1-score (`0.55` in the example).\n","\n","```\n","              precision    recall  f1-score   support\n","\n","         LOC       0.84      0.78      0.81      1837\n","        MISC       0.74      0.70      0.72       922\n","         ORG       0.21      0.76      0.32      1341\n","         PER       0.69      0.54      0.60      1842\n","\n","   micro avg       0.45      0.69      0.55      5942\n","   macro avg       0.62      0.70      0.61      5942\n","weighted avg       0.63      0.69      0.62      5942\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1651851350908,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"tPxysOE7rcnv"},"outputs":[],"source":["def ids_to_tags(y):\n","    y_tags = []\n","    for y_k in y:\n","        y_tags.append([y_enc.get_symbol(tag) for tag in y_k])\n","    return y_tags\n","\n","\n","def cr_eval(y, y_pred):\n","    y = ids_to_tags(y)\n","    y_pred = ids_to_tags(y_pred)\n","    return classification_report(y, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"pDyutCSb0vB5"},"source":["## Baseline method: Most Frequent Tag for each Token\n","It is always a good idea to implement a simple baseline method in order to have a reference performance.\n","In the following, we evaluate a baseline that, for each token, predicts the most frequent tag for this token in the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1415,"status":"ok","timestamp":1651851352318,"user":{"displayName":"Eraldo Fernandes","userId":"09824487528840751991"},"user_tz":-120},"id":"HUtVfME2k1lu","outputId":"31f866f2-f8d3-428c-b0df-1af9c297b7bb"},"outputs":[],"source":["# count occurrences of each word (row) x tag (column) on training data\n","freq_x_y = np.zeros((len(x_enc), len(y_enc)))\n","for x_k, y_k in zip(x_train, y_train):\n","    for x_t, y_t in zip(x_k, y_k):\n","        freq_x_y[x_t, y_t] += 1\n","\n","# most frequent tag for each word\n","most_freq_x = freq_x_y.argmax(axis=1)\n","\n","pred_y = []\n","for x_k in x_dev:\n","    pred_y.append(np.array([most_freq_x[x_t] for x_t in x_k]))\n","\n","print(f'Evaluation of \"Most Frequent per Token\":\\n\\n{cr_eval(y_dev, pred_y)}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### You can continue here with your answer + code (or program outside the notebook, both is fine)."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNnQhFlYSv36GJ6jZS5hcUw","collapsed_sections":[],"name":"HMM - NER Tagging.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}
